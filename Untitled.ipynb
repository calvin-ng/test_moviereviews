{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from os import listdir\n",
    "import regex as re\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r', encoding=\"utf8\")\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns = ['id','rating','sentiment', 'review'])\n",
    "# load all docs in a directory\n",
    "def process_docs(directory):\n",
    "\t# walk through all files in the folder\n",
    "    for i, filename in enumerate(listdir(directory)):\n",
    "\t\t# skip files that do not have the right extension\n",
    "        id_number = filename[:filename.find('_')]\n",
    "        rating = filename[filename.find('_')+1]\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "\t\t# create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        if (int(rating)<=4):\n",
    "            sent=0\n",
    "        else:\n",
    "            sent=1\n",
    "            \n",
    "        df.loc[i]=(id_number, rating, sent , load_doc(path))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_doc(directory):\n",
    "    dff=process_docs(directory)\n",
    "    for i in range(dff.shape[0]):\n",
    "        try:\n",
    "            doc\n",
    "        except NameError:\n",
    "            doc = [dff.iloc[0,3]]\n",
    "        else: \n",
    "            doc.append(dff.iloc[0,3])\n",
    "    \n",
    "    return doc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def TfidfVectorizer(strip_accents,lowercase,preprocessor,tokenizer,use_idf, norm, smooth_idf):\n",
    "#     count = CountVectorizer()\n",
    "#     docs = generate_doc('neg')\n",
    "#     bag = count.fit_transform(docs)\n",
    "#     # print(count.vocabulary_)\n",
    "#     #print(bag.toarray())\n",
    "\n",
    "#     np.set_printoptions(precision=2)\n",
    "#     tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "#     return tfidf.fit_transform(bag).toarray()\n",
    "\n",
    "def preprocessor(text):\n",
    "    text =re.sub('<[^>]*>','', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+','', text.lower()) + ''.join(emoticons).replace('-', '')\n",
    "    return text\n",
    "\n",
    "porter = PorterStemmer()\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_stemmer(text):\n",
    "    return[porter.stem(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "    lowercase=True,\n",
    "    preprocessor=preprocessor, # defined preprocessor in Data Cleaning\n",
    "    tokenizer=tokenizer_stemmer,\n",
    "    use_idf=True,\n",
    "    norm='l2',\n",
    "    smooth_idf=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(preprocessor=<function preprocessor at 0x1C2C6DF0>,\n",
      "                tokenizer=<function tokenizer_stemmer at 0x1C2C6E80>)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = process_docs('train')\n",
    "y = df.sentiment.values\n",
    "y=y.astype('int')\n",
    "X = tfidf.fit_transform(generate_doc('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    1.6s remaining:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    1.7s finished\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.5, shuffle=False)\n",
    "clf = LogisticRegressionCV(cv=5, scoring='accuracy', random_state=0, n_jobs=-1, verbose=3, max_iter=300).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7524752475247525"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
